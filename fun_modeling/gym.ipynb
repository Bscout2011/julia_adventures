{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OpenAI Gym Tutorial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent-Environment Loop\n",
    "\n",
    "Each `step` the agent chooses an `action`, and the environment returns an `observation` and `reward`. The actual four returned value are:\n",
    "\n",
    "- `observation` (**object**): an environment-specific object $o \\in \\mathcal O$ representing the observation of the environment.\n",
    "- `reward` (**float**): each action returns a reward.\n",
    "- `done` (**boolean**): when a terminal state is reached, time to `reset` the environment for the next episode.\n",
    "- `info` (**dict**): diagnostic info useful for debugging."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Actions\n",
    "\n",
    "Our first example randomly samples from the action *space*. Each environment comes with an `action_space` and `observation_space`. A `Discrete` space is a fixed set of non-negative numbers. The `Box` space is an $\\mathbf R^n$ dimensional vector of observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frozen Lake\n",
    "\n",
    "Consider a $4 \\times 4$ grid. The environment has 16 possible states $\\mathcal S$, and four actions $\\mathcal A$: `{0:LEFT, 1:DOWN, 2:RIGHT, 4:UP}`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\")\n",
    "env.reset()\n",
    "env.render()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "rewards = []\n",
    "actions = {\n",
    "    0: \"LEFT\",\n",
    "    1: \"DOWN\",\n",
    "    2: \"RIGHT\",\n",
    "    3: \"UP\"\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    r\"\"\"Policy evaluation function. Loop until state rewards are stable.\n",
    "    \n",
    "    Returns: \n",
    "        V (np.array): expected state value given an infinite horizon.\n",
    "        policy (np.array): best action for each state.\n",
    "\n",
    "    Args:\n",
    "        env (gym.env): gym environment.\n",
    "        gamma (float): future reward discount rate.\n",
    "        theta (float): stopping criterion.\n",
    "    \"\"\"\n",
    "    # Initialize state-value array\n",
    "    S_n = env.observation_space.n\n",
    "    V = np.zeros(S_n)\n",
    "    policy = np.ones(S_n) * -1\n",
    "    delta = np.zeros(S_n)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        # Loop through states\n",
    "        for s in env.P:\n",
    "            Vs = np.zeros(len(env.P[s]))\n",
    "            # Loop through available actions in each state\n",
    "            for a in env.P[s]:\n",
    "                # Loop though transition probabilities, next state, and rewards\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    Vs[a] = prob * (reward + gamma * V[next_state])\n",
    "            delta[s] = np.abs(V[s] - Vs.max())\n",
    "            V[s] = Vs.max()\n",
    "            policy[s] = Vs.argmax()\n",
    "        if np.all(delta < theta):\n",
    "            print(\"Value iteration complete after {} steps\".format(i))\n",
    "            break\n",
    "    return V, policy\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "env.P[14]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True)],\n",
       " 2: [(0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False)]}"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "env.render()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "V, policy = value_iteration(env, gamma=.9)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value iteration complete after 7 steps\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "print(np.array2string(V.reshape(4,4), precision=3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.001 0.003 0.009 0.003]\n",
      " [0.003 0.    0.03  0.   ]\n",
      " [0.009 0.03  0.1   0.   ]\n",
      " [0.    0.1   0.333 0.   ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "np.array([actions[a] for a in policy]).reshape(4,4)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['LEFT', 'DOWN', 'LEFT', 'UP'],\n",
       "       ['LEFT', 'LEFT', 'LEFT', 'LEFT'],\n",
       "       ['DOWN', 'LEFT', 'LEFT', 'LEFT'],\n",
       "       ['LEFT', 'DOWN', 'DOWN', 'LEFT']], dtype='<U4')"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "goal = 15\n",
    "for i_episode in range(20):\n",
    "    obs = env.reset()\n",
    "    for t in range(100):\n",
    "        # env.render()\n",
    "        # print(obs)\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, info = env.step(action)  # take a random action\n",
    "        if done:\n",
    "            if obs == goal:\n",
    "                print (\"Made the goal!\")\n",
    "            else:\n",
    "                print(\"Fell in the hole at ({},{})\".format(obs // 4, obs%4))\n",
    "            # print(\"Episode {} finished after {} timeteps. Reward {}.\".format(\n",
    "                # i_episode+1, t+1, reward))\n",
    "            break\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fell in the hole at (8,3)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (10,2)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (10,2)\n",
      "Fell in the hole at (8,3)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (14,3)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (10,2)\n",
      "Fell in the hole at (8,3)\n",
      "Fell in the hole at (8,3)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (12,1)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (8,3)\n",
      "Fell in the hole at (10,1)\n",
      "Fell in the hole at (8,3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}