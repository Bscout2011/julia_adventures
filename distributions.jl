### A Pluto.jl notebook ###
# v0.14.5

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# â•”â•â•¡ 2ba8b802-ec51-4b15-ad8d-c4a7e85e919b
begin
	import Pkg
	Pkg.add([
		Pkg.PackageSpec(name="PlutoUI", version="0.7"), 
		Pkg.PackageSpec(name="HypertextLiteral", version="0.5"), 
		Pkg.PackageSpec(name="Distributions", version="0.25"),
		Pkg.PackageSpec(name="Plots"),
		Pkg.PackageSpec(name="Contour")
	])

	using PlutoUI
	using HypertextLiteral
	using LinearAlgebra
	using Random 
	using Distributions
	using Plots
	using Contour
end

# â•”â•â•¡ 1eaa8b93-584b-4ea3-b1f9-40919eac516a
md"""
#### Intializing packages

_When running this notebook for the first time, this could take up to 15 minutes. Hang in there!_
"""

# â•”â•â•¡ 1764a60f-24c5-4f12-b924-d7b984e1a7a4
Random.seed!(123);  # set the seed for reproducibility

# â•”â•â•¡ 71b718ff-b9a3-432f-b0ce-dc9e40ff60dd
Resource("https://i.imgur.com/vf42nVH.jpeg")

# â•”â•â•¡ f13f63ec-b509-11eb-0ea5-45a1ac8c9d4e
md"""
# Two Class Discrimination

This page illustrates how to classify between two animals, say ğŸ® and ğŸ‘, based on two attributes, such as *weight* and *fluffiness*. 

We describe the problem using Bayes' Rule

\$$
P(ğŸ® \; | \; x) = \frac{p(x \; | \; ğŸ®) P(ğŸ®)}{p(x)}
\$$

where $x \in \mathbf R^2$ is a measurement sample of *weight* and *fluffiness*, $p(x \; | \; ğŸ®)$ is the likelihood distribution of each attribute given the animal is a cow, and $P(ğŸ®)$ is the prior probability we believe the animal is a cow. The problem is: given a new sample $x$, is it more likely to be a cow or sheep? Specifically,

\$$
\text{decide ğŸ® if} \quad p(x \; | ğŸ®) P(ğŸ®) > p(x \; | ğŸ‘) P(ğŸ‘), \quad \text{ otherwise, decide ğŸ‘.}
\$$

First we assume the likelihood distributions are multivariate gaussians with a mean and covariance 

\$$
\mathcal N (\mu, \Sigma) = \frac{1}{2\pi | \Sigma |^{1/2}} \exp \left[ -.5 (x - \mu)^T \Sigma^{-1} (x - \mu) \right].
\$$

*Hint*: just think of the gaussian curve as $\sim e^{-x^2}$.

Plugging in the distribution equation into the decision rule, taking the log of both sides, and rearranging terms reveals a *quadratic* discrimination formula

\$$
g_i (x) =  -.5 (x - \mu)^T \Sigma^{-1} (x - \mu) - .5 \ln | \Sigma | + \ln P({\omega})
\$$

where $g_i, \mu, \Sigma$ are respective for each class type (ğŸ®, ğŸ‘). 

Below, the interactive scrubbers allow you to change the distribution parameters and see how each of the terms affect the classification. Try out three special cases.

**Case 1**: \$\Sigma ğŸ® \, = \Sigma ğŸ‘ = \sigma I$: A euclidean classifier. Class distributions are spherical. Linear decision boundary. Prior scales boundary between class means.

**Case 2**: \$\Sigma ğŸ® \, = \Sigma ğŸ‘$. Distributions are equal ellipsoids. Equal covariances give the Mahalanbois linear classifier.  Distances in feature space are scaled by the covariance principal axes.

**Case 3**: $\Sigma ğŸ® \, \neq \Sigma ğŸ‘$. Quadratic classifier. Decision boundary has many degrees of freedom.
"""

# â•”â•â•¡ 4e8ecde0-f0a5-4eff-a858-a14b86b73dd5
let
Î¼_range = 0.1:.1:8
Ïƒ_range = 0.1:.1:5
cov_range = 0:.1:5
prior_range = 0.01:.1:.99
md"""
This is a "scrubbable matrix" -- click on the number and drag to change.	
	
Î¼ ğŸ® = ``(``	
 $(@bind a Scrubbable( Î¼_range; default=1.0))
 $(@bind b Scrubbable( Î¼_range; default=1.0))
``)``

Î¼ ğŸ‘ = 
``(``
$(@bind c Scrubbable(Î¼_range; default=4.0 ))
$(@bind d Scrubbable(Î¼_range; default=4.0))
``)``
	
Î£ ğŸ® = ``(``
	Ïƒâ‚ = $(@bind s11 Scrubbable(Ïƒ_range; default=1)),
	Ïƒâ‚‚ = $(@bind s12 Scrubbable(Ïƒ_range; default=1)),
	Ïƒâ‚â‚‚ = $(@bind s1cor Scrubbable(cov_range; default=0))
``)``
	
Î£ ğŸ‘ = ``(``
	Ïƒâ‚ = $(@bind s21 Scrubbable(Ïƒ_range; default=1)),
	Ïƒâ‚‚ = $(@bind s22 Scrubbable(Ïƒ_range; default=1)),
	Ïƒâ‚â‚‚ = $(@bind s2cor Scrubbable(cov_range; default=0))
``)``
	
Prior = $(@bind prior Slider(prior_range; default=.5, show_value=true))
	
Sample = ``(``	
 $(@bind s_x Scrubbable( Î¼_range; default=2.5))
 $(@bind s_y Scrubbable( Î¼_range; default=2.5))
``)``
	
**Re-run this cell to reset**
"""
end

# â•”â•â•¡ f45de1a7-b144-4ca5-9819-3a7455a1340c
function discriminant(x::AbstractVector, Î¼::AbstractVector, Î£::AbstractMatrix; prior=0.5)
	Î£_inv = Î£^-1
	W = -.5 * Î£_inv
	w = Î£_inv * Î¼
	wâ‚€ = -.5 * transpose(Î¼) * Î£_inv * Î¼ - .5 * log(det(Î£)) + log(prior)
	
	g = transpose(x) * W * x + transpose(w)*x + wâ‚€
	g
end

# â•”â•â•¡ d93ac03a-278d-4f70-a39c-6291e230073e
# Generate data
begin
	# discriminant(x; Î¼=[0., 0.], Î£=[1. 0.; 0. 1.], prior=.5) = -.5 * transpose(x - Î¼) * Î£â‚^-1 * (x - Î¼) - .5 * log(det(Î£)) + log(prior)
	
	nâ‚ = 3000 
	nâ‚‚ = 7000
	priorâ‚=prior
	priorâ‚‚=1 - prior

	Î¼â‚ = [a,b]
	Î¼â‚‚ = [c,d]
	Î£â‚ = [s11 s1cor
		  s1cor s12]
	Î£â‚‚ = [s21 s2cor
		  s2cor s22]
	
	# 	Assert covariances are positive definite
	det_Î£â‚ = det(Î£â‚)
	det_Î£â‚‚ = det(Î£â‚‚)
	if (det_Î£â‚ â‰¤ 0) || (det_Î£â‚‚ â‰¤ 0)
		DomainError([det_Î£â‚, det_Î£â‚‚], "Covariance must be positive definite.")
	end
	
	# Generate multivariate gaussian data
	distâ‚ = MvNormal(Î¼â‚, Î£â‚)
	distâ‚‚ = MvNormal(Î¼â‚‚, Î£â‚‚)
	dA = rand(distâ‚, nâ‚)
	dB = rand(distâ‚‚, nâ‚‚)
	
	
	grid = -2:.1:10
	zâ‚=[pdf(distâ‚, [x, y]) for y in grid, x in grid]
	zâ‚‚=[pdf(distâ‚‚, [x, y]) for y in grid, x in grid]
	gâ‚ = [discriminant([x, y], Î¼â‚, Î£â‚, prior=priorâ‚) for y in grid, x in grid]
	gâ‚‚ = [discriminant([x, y], Î¼â‚‚, Î£â‚‚, prior=priorâ‚‚) for y in grid, x in grid]
	g = gâ‚ - gâ‚‚
end;

# â•”â•â•¡ 7d8105ac-862e-48d8-9b1a-bd80387ac6e3
begin
	g_contour = lines(Contour.contour(grid, grid, g, 0))
	ys, xs = coordinates(g_contour[1])
	new_sample = [s_x, s_y]
	log_proba = discriminant(new_sample, Î¼â‚, Î£â‚, prior=priorâ‚) - discriminant(new_sample, Î¼â‚‚, Î£â‚‚, prior=priorâ‚‚)
	# scatter(dA[1,:], dA[2,:], 
	# 	alpha=0.1, 
	# 	xlabel="xâ‚", xlims=(-2, 10),
	# 	ylabel="xâ‚‚", ylims=(-2, 10),
	# 	label="Ï‰â‚")
	# scatter!(dB[1,:], dB[2,:], alpha=0.1, label="Ï‰â‚‚")
	Plots.contour(grid, grid, zâ‚, levels=3, color="blue", 
		xlabel="weight", ylabel="fluffiness", label="ğŸ®", legend=false)
	Plots.contour!(grid, grid, zâ‚‚, levels=3, color="red", label="ğŸ‘", legend=true)
	# contour!(grid, grid, g)
	plot!(xs, ys, color="black", label="Discriminant")
	scatter!([s_x], [s_y], label="Sample")
end

# â•”â•â•¡ 86acff24-c5cb-42f4-a1af-8da80d64f2b6
md"""
Sample classified as $(log_proba > 0 ? "ğŸ®" : "ğŸ‘"). Log Prob: $(log_proba)
"""

# â•”â•â•¡ 7ff8bc65-0864-4453-97de-6c88eefac295
md"""
## Reference

Some material on this website is based on "Computational Thinking, a live online Julia/Pluto textbook, https://computationalthinking.mit.edu"
"""

# â•”â•â•¡ Cell order:
# â•Ÿâ”€1eaa8b93-584b-4ea3-b1f9-40919eac516a
# â•Ÿâ”€2ba8b802-ec51-4b15-ad8d-c4a7e85e919b
# â•Ÿâ”€1764a60f-24c5-4f12-b924-d7b984e1a7a4
# â•Ÿâ”€71b718ff-b9a3-432f-b0ce-dc9e40ff60dd
# â•Ÿâ”€f13f63ec-b509-11eb-0ea5-45a1ac8c9d4e
# â•Ÿâ”€4e8ecde0-f0a5-4eff-a858-a14b86b73dd5
# â•Ÿâ”€f45de1a7-b144-4ca5-9819-3a7455a1340c
# â•Ÿâ”€d93ac03a-278d-4f70-a39c-6291e230073e
# â•Ÿâ”€7d8105ac-862e-48d8-9b1a-bd80387ac6e3
# â•Ÿâ”€86acff24-c5cb-42f4-a1af-8da80d64f2b6
# â•Ÿâ”€7ff8bc65-0864-4453-97de-6c88eefac295
